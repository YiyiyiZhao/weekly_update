## 总览
![Overview](./pic/20231031/1031.png)

## 进度
### 1. Model Test 目录
Server:10.21.4.51
![Img1](./pic/20231031/1.png)

### 2. Qwen Example
* image：  /nfs/zy/projects/benchmark/data/resized_images_1024_768/39.jpg
![Img3](./pic/20231031/3.png)
![Img2](./pic/20231031/2.png)
* ***Qwen-VL-Chat-Int4 模型输出***：The product you need is a banana. Based on the image, it can be seen that there are no other fruits 
or vegetables in your field of view. There is also no walkable passage in front of you. To obtain this item
, you should move to the left side and take about 5 steps forward. Then turn right and continue walking for
 around three steps. Finally, reach out with both hands and grab the bananas from the shelf at an appropria
te height.

* ***CogVLM-Chat-17B-Float16 模型输出***：: The banana is located in the middle of the image, on a shelf. You can take about 5 steps forward and then turn to your right. Then, you should walk towards the left side of the shelf with the orange sign that says "6" above it. Finally, reach up with both hands and grab the yellow bananas.



#### ToDo：
进一步分析结果
可借鉴PCA-EVAL类似？
![Img4](./pic/20231031/4.png)

### 3. Paper Reading
Github Link: <https://github.com/YiyiyiZhao/paper_reading/tree/main>

| **Name**                                                                     |
|------------------------------------------------------------------------------|
| Multimodal Foundation Models: From Specialists to General-Purpose Assistants |
| COGVLM: VISUAL EXPERT FOR LARGE LANGUAGE MODELS                              | 
| A Survey on Large Language Model based Autonomous Agents                     |
| Cognitive Architectures for Language Agents|

## Todo:
1. Review 更新补充
2. Test的结果跑出来
